{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook takes the first example from rnaglib and runs the model with different optimizers (Adam and Adagrad) as well as test different lr (0.001 and 0.5) and plots the loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_supervised function from learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASGD', 'Adadelta', 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'LBFGS', 'NAdam', 'Optimizer', 'RAdam', 'RMSprop', 'Rprop', 'SGD', 'SparseAdam', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_functional', '_multi_tensor', 'lr_scheduler', 'swa_utils']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(dir(torch.optim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was found and not overwritten\n",
      "Train Epoch: 1 [1/86 (1%)]\tLoss: 3.437745  Time: 0.73\n",
      "Train Epoch: 1 [21/86 (24%)]\tLoss: 1.894585  Time: 5.95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# Finally get the training going\u001b[39;00m\n\u001b[1;32m     31\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(classifier_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m learn\u001b[39m.\u001b[39;49mtrain_supervised(model\u001b[39m=\u001b[39;49mclassifier_model,\n\u001b[1;32m     33\u001b[0m                        optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     34\u001b[0m                        train_loader\u001b[39m=\u001b[39;49mtrain_loader)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/learning/learn.py:131\u001b[0m, in \u001b[0;36mtrain_supervised\u001b[0;34m(model, optimizer, train_loader, learning_routine)\u001b[0m\n\u001b[1;32m    128\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    129\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    132\u001b[0m     \u001b[39m# Get data on the devices\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     graph \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    134\u001b[0m     graph \u001b[39m=\u001b[39m learning_utils\u001b[39m.\u001b[39msend_graph_to_device(graph, device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py:95\u001b[0m, in \u001b[0;36mRNADataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m rna_graph \u001b[39m=\u001b[39m load_graph(g_path)\n\u001b[1;32m     91\u001b[0m rna_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mrna_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_graphs[idx],\n\u001b[1;32m     92\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mrna\u001b[39m\u001b[39m'\u001b[39m: rna_graph,\n\u001b[1;32m     93\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m: g_path\n\u001b[1;32m     94\u001b[0m             }\n\u001b[0;32m---> 95\u001b[0m features_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_features(rna_dict)\n\u001b[1;32m     96\u001b[0m \u001b[39m# apply representations to the res_dict\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m# each is a callable that updates the res_dict\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m rep \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentations:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py:182\u001b[0m, in \u001b[0;36mRNADataset.compute_features\u001b[0;34m(self, rna_dict)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39m# Get Node labels\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_features_parser) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 182\u001b[0m     feature_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_nt_encoding(graph, encode_feature\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    183\u001b[0m     features_dict[\u001b[39m'\u001b[39m\u001b[39mnt_features\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m feature_encoding\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_target_parser) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py:147\u001b[0m, in \u001b[0;36mRNADataset.get_nt_encoding\u001b[0;34m(self, g, encode_feature)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     node_feature \u001b[39m=\u001b[39m attrs[feature]\n\u001b[0;32m--> 147\u001b[0m     node_feature_encoding \u001b[39m=\u001b[39m feature_encoder\u001b[39m.\u001b[39;49mencode(node_feature)\n\u001b[1;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     node_feature_encoding \u001b[39m=\u001b[39m feature_encoder\u001b[39m.\u001b[39mencode_default()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import torch\n",
    "\n",
    "from rnaglib.learning import models, learn\n",
    "from rnaglib.data_loading import rna_dataset, rna_loader\n",
    "from rnaglib.representations import GraphRepresentation\n",
    "\n",
    "\"\"\"\n",
    "This script just shows a first very basic example : learn binding protein preferences \n",
    "from the nucleotide types and the graph structure\n",
    "\n",
    "To do so, we choose our data, create a data loader around it, build a RGCN model and train it.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose the data, features and targets to use and GET THE DATA GOING\n",
    "    node_features = ['nt_code']\n",
    "    node_target = ['binding_protein']\n",
    "    graph_rep = GraphRepresentation(framework='dgl')\n",
    "    supervised_dataset = rna_dataset.RNADataset(nt_features=node_features, nt_targets=node_target,\n",
    "                                                representations=[graph_rep])\n",
    "    train_loader, validation_loader, test_loader = rna_loader.get_loader(dataset=supervised_dataset)\n",
    "\n",
    "    # Define a model, we first embed our data in 10 dimensions, and then add one classification\n",
    "    input_dim, target_dim = supervised_dataset.input_dim, supervised_dataset.output_dim\n",
    "    embedder_model = models.Embedder(dims=[10, 10], infeatures_dim=input_dim)\n",
    "    classifier_model = models.Classifier(embedder=embedder_model, classif_dims=[target_dim])\n",
    "\n",
    "    # Finally get the training going\n",
    "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.001)\n",
    "    learn.train_supervised(model=classifier_model,\n",
    "                           optimizer=optimizer,\n",
    "                           train_loader=train_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I used another optimizer, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was found and not overwritten\n",
      "Train Epoch: 1 [1/84 (1%)]\tLoss: 5.522629  Time: 0.16\n",
      "Train Epoch: 1 [21/84 (25%)]\tLoss: 3.042251  Time: 4.21\n",
      "Train Epoch: 1 [41/84 (49%)]\tLoss: 4.284922  Time: 7.47\n",
      "Train Epoch: 1 [61/84 (73%)]\tLoss: 4.211458  Time: 10.04\n",
      "Train Epoch: 1 [81/84 (96%)]\tLoss: 2.582106  Time: 12.75\n",
      "Train Epoch: 2 [1/84 (1%)]\tLoss: 2.689603  Time: 13.20\n",
      "Train Epoch: 2 [21/84 (25%)]\tLoss: 1.907818  Time: 16.29\n",
      "Train Epoch: 2 [41/84 (49%)]\tLoss: 2.069331  Time: 18.91\n",
      "Train Epoch: 2 [61/84 (73%)]\tLoss: 2.091504  Time: 22.54\n",
      "Train Epoch: 2 [81/84 (96%)]\tLoss: 2.599396  Time: 25.93\n",
      "Train Epoch: 3 [1/84 (1%)]\tLoss: 2.370876  Time: 26.14\n",
      "Train Epoch: 3 [21/84 (25%)]\tLoss: 1.614398  Time: 28.90\n",
      "Train Epoch: 3 [41/84 (49%)]\tLoss: 1.629896  Time: 31.80\n",
      "Train Epoch: 3 [61/84 (73%)]\tLoss: 1.099264  Time: 35.75\n",
      "Train Epoch: 3 [81/84 (96%)]\tLoss: 1.319499  Time: 38.47\n",
      "Train Epoch: 4 [1/84 (1%)]\tLoss: 1.522890  Time: 39.51\n",
      "Train Epoch: 4 [21/84 (25%)]\tLoss: 1.810334  Time: 41.92\n",
      "Train Epoch: 4 [41/84 (49%)]\tLoss: 0.732477  Time: 46.04\n",
      "Train Epoch: 4 [61/84 (73%)]\tLoss: 1.693358  Time: 47.66\n",
      "Train Epoch: 4 [81/84 (96%)]\tLoss: 1.629612  Time: 50.67\n",
      "Train Epoch: 5 [1/84 (1%)]\tLoss: 1.627078  Time: 52.87\n",
      "Train Epoch: 5 [21/84 (25%)]\tLoss: 1.463288  Time: 56.62\n",
      "Train Epoch: 5 [41/84 (49%)]\tLoss: 1.687408  Time: 59.76\n",
      "Train Epoch: 5 [61/84 (73%)]\tLoss: 1.374859  Time: 63.11\n",
      "Train Epoch: 5 [81/84 (96%)]\tLoss: 1.491921  Time: 64.90\n",
      "Train Epoch: 6 [1/84 (1%)]\tLoss: 1.245369  Time: 65.49\n",
      "Train Epoch: 6 [21/84 (25%)]\tLoss: 1.212729  Time: 68.66\n",
      "Train Epoch: 6 [41/84 (49%)]\tLoss: 1.278140  Time: 70.71\n",
      "Train Epoch: 6 [61/84 (73%)]\tLoss: 1.135817  Time: 74.46\n",
      "Train Epoch: 6 [81/84 (96%)]\tLoss: 1.099676  Time: 77.18\n",
      "Train Epoch: 7 [1/84 (1%)]\tLoss: 1.114234  Time: 78.17\n",
      "Train Epoch: 7 [21/84 (25%)]\tLoss: 1.160043  Time: 81.41\n",
      "Train Epoch: 7 [41/84 (49%)]\tLoss: 0.870234  Time: 84.92\n",
      "Train Epoch: 7 [61/84 (73%)]\tLoss: 0.772054  Time: 87.11\n",
      "Train Epoch: 7 [81/84 (96%)]\tLoss: 1.210569  Time: 89.85\n",
      "Train Epoch: 8 [1/84 (1%)]\tLoss: 1.028058  Time: 90.46\n",
      "Train Epoch: 8 [21/84 (25%)]\tLoss: 1.045462  Time: 94.63\n",
      "Train Epoch: 8 [41/84 (49%)]\tLoss: 1.026086  Time: 98.10\n",
      "Train Epoch: 8 [61/84 (73%)]\tLoss: 1.064984  Time: 100.01\n",
      "Train Epoch: 8 [81/84 (96%)]\tLoss: 0.969673  Time: 102.71\n",
      "Train Epoch: 9 [1/84 (1%)]\tLoss: 0.920215  Time: 103.88\n",
      "Train Epoch: 9 [21/84 (25%)]\tLoss: 1.239385  Time: 106.03\n",
      "Train Epoch: 9 [41/84 (49%)]\tLoss: 1.049788  Time: 109.88\n",
      "Train Epoch: 9 [61/84 (73%)]\tLoss: 1.071666  Time: 112.77\n",
      "Train Epoch: 9 [81/84 (96%)]\tLoss: 0.868412  Time: 115.33\n",
      "Train Epoch: 10 [1/84 (1%)]\tLoss: 1.104694  Time: 116.46\n",
      "Train Epoch: 10 [21/84 (25%)]\tLoss: 1.043822  Time: 122.65\n",
      "Train Epoch: 10 [41/84 (49%)]\tLoss: 0.872286  Time: 123.87\n",
      "Train Epoch: 10 [61/84 (73%)]\tLoss: 0.819345  Time: 126.50\n",
      "Train Epoch: 10 [81/84 (96%)]\tLoss: 0.761958  Time: 129.35\n",
      "Train Epoch: 11 [1/84 (1%)]\tLoss: 0.844594  Time: 129.49\n",
      "Train Epoch: 11 [21/84 (25%)]\tLoss: 0.856505  Time: 132.76\n",
      "Train Epoch: 11 [41/84 (49%)]\tLoss: 0.896350  Time: 135.05\n",
      "Train Epoch: 11 [61/84 (73%)]\tLoss: 0.913889  Time: 139.65\n",
      "Train Epoch: 11 [81/84 (96%)]\tLoss: 0.919953  Time: 141.63\n",
      "Train Epoch: 12 [1/84 (1%)]\tLoss: 0.922962  Time: 142.35\n",
      "Train Epoch: 12 [21/84 (25%)]\tLoss: 0.881119  Time: 145.25\n",
      "Train Epoch: 12 [41/84 (49%)]\tLoss: 0.680274  Time: 148.66\n",
      "Train Epoch: 12 [61/84 (73%)]\tLoss: 0.936574  Time: 151.14\n",
      "Train Epoch: 12 [81/84 (96%)]\tLoss: 0.793474  Time: 154.56\n",
      "Train Epoch: 13 [1/84 (1%)]\tLoss: 0.602213  Time: 155.73\n",
      "Train Epoch: 13 [21/84 (25%)]\tLoss: 0.678967  Time: 157.77\n",
      "Train Epoch: 13 [41/84 (49%)]\tLoss: 0.831525  Time: 159.94\n",
      "Train Epoch: 13 [61/84 (73%)]\tLoss: 0.684758  Time: 163.44\n",
      "Train Epoch: 13 [81/84 (96%)]\tLoss: 0.847026  Time: 168.42\n",
      "Train Epoch: 14 [1/84 (1%)]\tLoss: 0.562780  Time: 169.12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# Finally get the training going\u001b[39;00m\n\u001b[1;32m     31\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdagrad(classifier_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m learn\u001b[39m.\u001b[39;49mtrain_supervised(model\u001b[39m=\u001b[39;49mclassifier_model,\n\u001b[1;32m     33\u001b[0m                        optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     34\u001b[0m                        train_loader\u001b[39m=\u001b[39;49mtrain_loader)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/learning/learn.py:131\u001b[0m, in \u001b[0;36mtrain_supervised\u001b[0;34m(model, optimizer, train_loader, learning_routine)\u001b[0m\n\u001b[1;32m    128\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    129\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    132\u001b[0m     \u001b[39m# Get data on the devices\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     graph \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    134\u001b[0m     graph \u001b[39m=\u001b[39m learning_utils\u001b[39m.\u001b[39msend_graph_to_device(graph, device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py:95\u001b[0m, in \u001b[0;36mRNADataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m rna_graph \u001b[39m=\u001b[39m load_graph(g_path)\n\u001b[1;32m     91\u001b[0m rna_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mrna_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_graphs[idx],\n\u001b[1;32m     92\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mrna\u001b[39m\u001b[39m'\u001b[39m: rna_graph,\n\u001b[1;32m     93\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m: g_path\n\u001b[1;32m     94\u001b[0m             }\n\u001b[0;32m---> 95\u001b[0m features_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_features(rna_dict)\n\u001b[1;32m     96\u001b[0m \u001b[39m# apply representations to the res_dict\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m# each is a callable that updates the res_dict\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m rep \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentations:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py:185\u001b[0m, in \u001b[0;36mRNADataset.compute_features\u001b[0;34m(self, rna_dict)\u001b[0m\n\u001b[1;32m    183\u001b[0m     features_dict[\u001b[39m'\u001b[39m\u001b[39mnt_features\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m feature_encoding\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_target_parser) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     target_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_nt_encoding(graph, encode_feature\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    186\u001b[0m     features_dict[\u001b[39m'\u001b[39m\u001b[39mnt_targets\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m target_encoding\n\u001b[1;32m    187\u001b[0m \u001b[39mreturn\u001b[39;00m features_dict\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py:147\u001b[0m, in \u001b[0;36mRNADataset.get_nt_encoding\u001b[0;34m(self, g, encode_feature)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     node_feature \u001b[39m=\u001b[39m attrs[feature]\n\u001b[0;32m--> 147\u001b[0m     node_feature_encoding \u001b[39m=\u001b[39m feature_encoder\u001b[39m.\u001b[39;49mencode(node_feature)\n\u001b[1;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     node_feature_encoding \u001b[39m=\u001b[39m feature_encoder\u001b[39m.\u001b[39mencode_default()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/utils/feature_maps.py:93\u001b[0m, in \u001b[0;36mBoolEncoder.encode\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mAssign encoding of `value` according to known possible values.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[39m:param value: The value to encode. If missing the default value (False by default) is produced.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_default()\n\u001b[1;32m     94\u001b[0m \u001b[39m# Sometimes we encode other stuff as booleans. Then if it's here return True, else False\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/utils/feature_maps.py:102\u001b[0m, in \u001b[0;36mBoolEncoder.encode_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_default\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 102\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault_value], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import torch\n",
    "\n",
    "from rnaglib.learning import models, learn\n",
    "from rnaglib.data_loading import rna_dataset, rna_loader\n",
    "from rnaglib.representations import GraphRepresentation\n",
    "\n",
    "\"\"\"\n",
    "This script just shows a first very basic example : learn binding protein preferences \n",
    "from the nucleotide types and the graph structure\n",
    "\n",
    "To do so, we choose our data, create a data loader around it, build a RGCN model and train it.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose the data, features and targets to use and GET THE DATA GOING\n",
    "    node_features = ['nt_code']\n",
    "    node_target = ['binding_protein']\n",
    "    graph_rep = GraphRepresentation(framework='dgl')\n",
    "    supervised_dataset = rna_dataset.RNADataset(nt_features=node_features, nt_targets=node_target,\n",
    "                                                representations=[graph_rep])\n",
    "    train_loader, validation_loader, test_loader = rna_loader.get_loader(dataset=supervised_dataset)\n",
    "\n",
    "    # Define a model, we first embed our data in 10 dimensions, and then add one classification\n",
    "    input_dim, target_dim = supervised_dataset.input_dim, supervised_dataset.output_dim\n",
    "    embedder_model = models.Embedder(dims=[10, 10], infeatures_dim=input_dim)\n",
    "    classifier_model = models.Classifier(embedder=embedder_model, classif_dims=[target_dim])\n",
    "\n",
    "    # Finally get the training going\n",
    "    optimizer = torch.optim.Adagrad(classifier_model.parameters(), lr=0.001)\n",
    "    learn.train_supervised(model=classifier_model,\n",
    "                           optimizer=optimizer,\n",
    "                           train_loader=train_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I used Adam optimizer but increases the lr to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was found and not overwritten\n",
      "Train Epoch: 1 [1/84 (1%)]\tLoss: 5.402839  Time: 0.05\n",
      "Train Epoch: 1 [21/84 (25%)]\tLoss: 12.969813  Time: 2.70\n",
      "Train Epoch: 1 [41/84 (49%)]\tLoss: 1.762604  Time: 7.51\n",
      "Train Epoch: 1 [61/84 (73%)]\tLoss: 0.182405  Time: 9.76\n",
      "Train Epoch: 1 [81/84 (96%)]\tLoss: 1.822942  Time: 12.31\n",
      "Train Epoch: 2 [1/84 (1%)]\tLoss: 1.642954  Time: 13.43\n",
      "Train Epoch: 2 [21/84 (25%)]\tLoss: 0.278313  Time: 16.16\n",
      "Train Epoch: 2 [41/84 (49%)]\tLoss: 0.315592  Time: 19.97\n",
      "Train Epoch: 2 [61/84 (73%)]\tLoss: 0.270255  Time: 22.57\n",
      "Train Epoch: 2 [81/84 (96%)]\tLoss: 0.842609  Time: 25.52\n",
      "Train Epoch: 3 [1/84 (1%)]\tLoss: 0.269375  Time: 26.10\n",
      "Train Epoch: 3 [21/84 (25%)]\tLoss: 1.735049  Time: 29.43\n",
      "Train Epoch: 3 [41/84 (49%)]\tLoss: 0.340293  Time: 31.41\n",
      "Train Epoch: 3 [61/84 (73%)]\tLoss: 1.557672  Time: 36.48\n",
      "Train Epoch: 3 [81/84 (96%)]\tLoss: 3.097146  Time: 39.15\n",
      "Train Epoch: 4 [1/84 (1%)]\tLoss: 1.288557  Time: 39.25\n",
      "Train Epoch: 4 [21/84 (25%)]\tLoss: 2.584188  Time: 43.24\n",
      "Train Epoch: 4 [41/84 (49%)]\tLoss: 0.360781  Time: 44.74\n",
      "Train Epoch: 4 [61/84 (73%)]\tLoss: 1.689546  Time: 48.23\n",
      "Train Epoch: 4 [81/84 (96%)]\tLoss: 0.452565  Time: 52.42\n",
      "Train Epoch: 5 [1/84 (1%)]\tLoss: 0.427503  Time: 52.57\n",
      "Train Epoch: 5 [21/84 (25%)]\tLoss: 0.714570  Time: 55.64\n",
      "Train Epoch: 5 [41/84 (49%)]\tLoss: 0.327556  Time: 57.81\n",
      "Train Epoch: 5 [61/84 (73%)]\tLoss: 0.673237  Time: 60.77\n",
      "Train Epoch: 5 [81/84 (96%)]\tLoss: 0.943326  Time: 65.72\n",
      "Train Epoch: 6 [1/84 (1%)]\tLoss: 0.686114  Time: 66.88\n",
      "Train Epoch: 6 [21/84 (25%)]\tLoss: 0.282841  Time: 69.07\n",
      "Train Epoch: 6 [41/84 (49%)]\tLoss: 0.302449  Time: 72.91\n",
      "Train Epoch: 6 [61/84 (73%)]\tLoss: 0.251868  Time: 75.74\n",
      "Train Epoch: 6 [81/84 (96%)]\tLoss: 0.322929  Time: 78.68\n",
      "Train Epoch: 7 [1/84 (1%)]\tLoss: 2.438280  Time: 78.88\n",
      "Train Epoch: 7 [21/84 (25%)]\tLoss: 0.817143  Time: 82.82\n",
      "Train Epoch: 7 [41/84 (49%)]\tLoss: 0.232165  Time: 84.66\n",
      "Train Epoch: 7 [61/84 (73%)]\tLoss: 0.820415  Time: 88.51\n",
      "Train Epoch: 7 [81/84 (96%)]\tLoss: 0.217504  Time: 91.69\n",
      "Train Epoch: 8 [1/84 (1%)]\tLoss: 0.419905  Time: 91.84\n",
      "Train Epoch: 8 [21/84 (25%)]\tLoss: 0.779514  Time: 96.99\n",
      "Train Epoch: 8 [41/84 (49%)]\tLoss: 0.254954  Time: 98.72\n",
      "Train Epoch: 8 [61/84 (73%)]\tLoss: 0.591718  Time: 101.51\n",
      "Train Epoch: 8 [81/84 (96%)]\tLoss: 0.376443  Time: 104.85\n",
      "Train Epoch: 9 [1/84 (1%)]\tLoss: 0.970286  Time: 105.44\n",
      "Train Epoch: 9 [21/84 (25%)]\tLoss: 0.263948  Time: 107.73\n",
      "Train Epoch: 9 [41/84 (49%)]\tLoss: 0.237215  Time: 110.77\n",
      "Train Epoch: 9 [61/84 (73%)]\tLoss: 0.266171  Time: 113.65\n",
      "Train Epoch: 9 [81/84 (96%)]\tLoss: 0.279746  Time: 117.65\n",
      "Train Epoch: 10 [1/84 (1%)]\tLoss: 0.289780  Time: 118.47\n",
      "Train Epoch: 10 [21/84 (25%)]\tLoss: 0.324558  Time: 122.02\n",
      "Train Epoch: 10 [41/84 (49%)]\tLoss: 0.968718  Time: 126.24\n",
      "Train Epoch: 10 [61/84 (73%)]\tLoss: 0.319142  Time: 128.42\n",
      "Train Epoch: 10 [81/84 (96%)]\tLoss: 0.230831  Time: 131.57\n",
      "Train Epoch: 11 [1/84 (1%)]\tLoss: 0.297878  Time: 131.67\n",
      "Train Epoch: 11 [21/84 (25%)]\tLoss: 0.351753  Time: 135.15\n",
      "Train Epoch: 11 [41/84 (49%)]\tLoss: 0.269821  Time: 138.67\n",
      "Train Epoch: 11 [61/84 (73%)]\tLoss: 0.265287  Time: 141.21\n",
      "Train Epoch: 11 [81/84 (96%)]\tLoss: 0.262085  Time: 144.56\n",
      "Train Epoch: 12 [1/84 (1%)]\tLoss: 0.205993  Time: 144.88\n",
      "Train Epoch: 12 [21/84 (25%)]\tLoss: 0.637181  Time: 149.18\n",
      "Train Epoch: 12 [41/84 (49%)]\tLoss: 0.304466  Time: 151.11\n",
      "Train Epoch: 12 [61/84 (73%)]\tLoss: 0.276871  Time: 154.17\n",
      "Train Epoch: 12 [81/84 (96%)]\tLoss: 0.228391  Time: 157.89\n",
      "Train Epoch: 13 [1/84 (1%)]\tLoss: 0.343104  Time: 158.02\n",
      "Train Epoch: 13 [21/84 (25%)]\tLoss: 0.488650  Time: 161.29\n",
      "Train Epoch: 13 [41/84 (49%)]\tLoss: 0.212795  Time: 164.65\n",
      "Train Epoch: 13 [61/84 (73%)]\tLoss: 0.350699  Time: 166.82\n",
      "Train Epoch: 13 [81/84 (96%)]\tLoss: 0.299821  Time: 170.54\n",
      "Train Epoch: 14 [1/84 (1%)]\tLoss: 0.308211  Time: 171.21\n",
      "Train Epoch: 14 [21/84 (25%)]\tLoss: 0.246879  Time: 174.98\n",
      "Train Epoch: 14 [41/84 (49%)]\tLoss: 0.255480  Time: 179.21\n",
      "Train Epoch: 14 [61/84 (73%)]\tLoss: 0.482490  Time: 181.57\n",
      "Train Epoch: 14 [81/84 (96%)]\tLoss: 0.319953  Time: 183.57\n",
      "Train Epoch: 15 [1/84 (1%)]\tLoss: 0.266809  Time: 183.84\n",
      "Train Epoch: 15 [21/84 (25%)]\tLoss: 0.283279  Time: 185.45\n",
      "Train Epoch: 15 [41/84 (49%)]\tLoss: 0.184033  Time: 188.36\n",
      "Train Epoch: 15 [61/84 (73%)]\tLoss: 0.293898  Time: 192.20\n",
      "Train Epoch: 15 [81/84 (96%)]\tLoss: 0.339789  Time: 196.17\n",
      "Train Epoch: 16 [1/84 (1%)]\tLoss: 0.224991  Time: 197.12\n",
      "Train Epoch: 16 [21/84 (25%)]\tLoss: 0.265540  Time: 200.90\n",
      "Train Epoch: 16 [41/84 (49%)]\tLoss: 0.591412  Time: 202.86\n",
      "Train Epoch: 16 [61/84 (73%)]\tLoss: 0.243169  Time: 206.12\n",
      "Train Epoch: 16 [81/84 (96%)]\tLoss: 0.294499  Time: 209.21\n",
      "Train Epoch: 17 [1/84 (1%)]\tLoss: 0.180641  Time: 209.80\n",
      "Train Epoch: 17 [21/84 (25%)]\tLoss: 0.226439  Time: 212.10\n",
      "Train Epoch: 17 [41/84 (49%)]\tLoss: 0.403071  Time: 216.39\n",
      "Train Epoch: 17 [61/84 (73%)]\tLoss: 0.237499  Time: 219.42\n",
      "Train Epoch: 17 [81/84 (96%)]\tLoss: 0.251575  Time: 222.52\n",
      "Train Epoch: 18 [1/84 (1%)]\tLoss: 0.261170  Time: 222.65\n",
      "Train Epoch: 18 [21/84 (25%)]\tLoss: 0.275595  Time: 225.03\n",
      "Train Epoch: 18 [41/84 (49%)]\tLoss: 0.229306  Time: 229.89\n",
      "Train Epoch: 18 [61/84 (73%)]\tLoss: 0.269969  Time: 232.07\n",
      "Train Epoch: 18 [81/84 (96%)]\tLoss: 0.461830  Time: 235.41\n",
      "Train Epoch: 19 [1/84 (1%)]\tLoss: 0.275160  Time: 235.55\n",
      "Train Epoch: 19 [21/84 (25%)]\tLoss: 0.241721  Time: 238.34\n",
      "Train Epoch: 19 [41/84 (49%)]\tLoss: 0.246536  Time: 242.38\n",
      "Train Epoch: 19 [61/84 (73%)]\tLoss: 0.256871  Time: 245.56\n",
      "Train Epoch: 19 [81/84 (96%)]\tLoss: 0.241727  Time: 248.20\n",
      "Train Epoch: 20 [1/84 (1%)]\tLoss: 0.265320  Time: 248.45\n",
      "Train Epoch: 20 [21/84 (25%)]\tLoss: 0.254274  Time: 250.90\n",
      "Train Epoch: 20 [41/84 (49%)]\tLoss: 0.235344  Time: 253.37\n",
      "Train Epoch: 20 [61/84 (73%)]\tLoss: 0.258804  Time: 256.44\n",
      "Train Epoch: 20 [81/84 (96%)]\tLoss: 0.257040  Time: 261.07\n",
      "Train Epoch: 21 [1/84 (1%)]\tLoss: 0.496019  Time: 261.71\n",
      "Train Epoch: 21 [21/84 (25%)]\tLoss: 0.310304  Time: 264.04\n",
      "Train Epoch: 21 [41/84 (49%)]\tLoss: 0.251986  Time: 266.85\n",
      "Train Epoch: 21 [61/84 (73%)]\tLoss: 0.243377  Time: 271.01\n",
      "Train Epoch: 21 [81/84 (96%)]\tLoss: 0.307759  Time: 273.45\n",
      "Train Epoch: 22 [1/84 (1%)]\tLoss: 0.314028  Time: 274.76\n",
      "Train Epoch: 22 [21/84 (25%)]\tLoss: 0.250711  Time: 278.30\n",
      "Train Epoch: 22 [41/84 (49%)]\tLoss: 0.240204  Time: 282.95\n",
      "Train Epoch: 22 [61/84 (73%)]\tLoss: 0.244805  Time: 284.58\n",
      "Train Epoch: 22 [81/84 (96%)]\tLoss: 0.087767  Time: 287.24\n",
      "Train Epoch: 23 [1/84 (1%)]\tLoss: 0.303426  Time: 287.40\n",
      "Train Epoch: 23 [21/84 (25%)]\tLoss: 0.286364  Time: 291.19\n",
      "Train Epoch: 23 [41/84 (49%)]\tLoss: 0.304471  Time: 293.78\n",
      "Train Epoch: 23 [61/84 (73%)]\tLoss: 0.282671  Time: 296.95\n",
      "Train Epoch: 23 [81/84 (96%)]\tLoss: 0.240297  Time: 299.85\n",
      "Train Epoch: 24 [1/84 (1%)]\tLoss: 0.258880  Time: 300.72\n",
      "Train Epoch: 24 [21/84 (25%)]\tLoss: 0.285864  Time: 304.15\n",
      "Train Epoch: 24 [41/84 (49%)]\tLoss: 0.236605  Time: 307.03\n",
      "Train Epoch: 24 [61/84 (73%)]\tLoss: 0.264912  Time: 310.34\n",
      "Train Epoch: 24 [81/84 (96%)]\tLoss: 0.246842  Time: 313.94\n",
      "Train Epoch: 25 [1/84 (1%)]\tLoss: 0.201562  Time: 314.16\n",
      "Train Epoch: 25 [21/84 (25%)]\tLoss: 0.240533  Time: 316.97\n",
      "Train Epoch: 25 [41/84 (49%)]\tLoss: 0.235855  Time: 319.80\n",
      "Train Epoch: 25 [61/84 (73%)]\tLoss: 0.288169  Time: 323.76\n",
      "Train Epoch: 25 [81/84 (96%)]\tLoss: 0.206233  Time: 327.10\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import torch\n",
    "\n",
    "from rnaglib.learning import models, learn\n",
    "from rnaglib.data_loading import rna_dataset, rna_loader\n",
    "from rnaglib.representations import GraphRepresentation\n",
    "\n",
    "\"\"\"\n",
    "This script just shows a first very basic example : learn binding protein preferences \n",
    "from the nucleotide types and the graph structure\n",
    "\n",
    "To do so, we choose our data, create a data loader around it, build a RGCN model and train it.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose the data, features and targets to use and GET THE DATA GOING\n",
    "    node_features = ['nt_code']\n",
    "    node_target = ['binding_protein']\n",
    "    graph_rep = GraphRepresentation(framework='dgl')\n",
    "    supervised_dataset = rna_dataset.RNADataset(nt_features=node_features, nt_targets=node_target,\n",
    "                                                representations=[graph_rep])\n",
    "    train_loader, validation_loader, test_loader = rna_loader.get_loader(dataset=supervised_dataset)\n",
    "\n",
    "    # Define a model, we first embed our data in 10 dimensions, and then add one classification\n",
    "    input_dim, target_dim = supervised_dataset.input_dim, supervised_dataset.output_dim\n",
    "    embedder_model = models.Embedder(dims=[10, 10], infeatures_dim=input_dim)\n",
    "    classifier_model = models.Classifier(embedder=embedder_model, classif_dims=[target_dim])\n",
    "\n",
    "    # Finally get the training going\n",
    "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.5)\n",
    "    learn.train_supervised(model=classifier_model,\n",
    "                           optimizer=optimizer,\n",
    "                           train_loader=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was found and not overwritten\n",
      "Train Epoch: 1 [1/85 (1%)]\tLoss: 2.807066  Time: 0.28\n",
      "Train Epoch: 1 [21/85 (25%)]\tLoss: 3.142740  Time: 6.60\n",
      "Train Epoch: 1 [41/85 (48%)]\tLoss: 3.185879  Time: 12.48\n",
      "Train Epoch: 1 [61/85 (72%)]\tLoss: 1.503721  Time: 16.36\n",
      "Train Epoch: 1 [81/85 (95%)]\tLoss: 0.760554  Time: 20.90\n",
      "Train Epoch: 2 [1/85 (1%)]\tLoss: 1.403902  Time: 22.10\n",
      "Train Epoch: 2 [21/85 (25%)]\tLoss: 0.732922  Time: 25.67\n",
      "Train Epoch: 2 [41/85 (48%)]\tLoss: 0.310032  Time: 30.63\n",
      "Train Epoch: 2 [61/85 (72%)]\tLoss: 1.966602  Time: 37.85\n",
      "Train Epoch: 2 [81/85 (95%)]\tLoss: 0.585249  Time: 42.66\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from rnaglib.config.graph_keys import GRAPH_KEYS, TOOL\n",
    "from rnaglib.utils import misc\n",
    "from rnaglib.learning import learning_utils\n",
    "import matplotlib.pyplot as plt\n",
    "def train_supervised(model,\n",
    "                     optimizer,\n",
    "                     train_loader,\n",
    "                     learning_routine=learning_utils.LearningRoutine()):\n",
    "    \"\"\"\n",
    "    Performs the entire training routine for a supervised task\n",
    "\n",
    "    :param model: The model to train\n",
    "    :param optimizer: the optimizer to use (eg SGD or Adam)\n",
    "    :param train_loader: The loader to use for training, as defined in data_loading/GraphLoader\n",
    "    :param learning_routine: A LearningRoutine object, if we want to also use a validation phase and early stopping\n",
    "\n",
    "    :return: The best loss obtained\n",
    "    \"\"\"\n",
    "    device = model.current_device\n",
    "\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "    # for epoch in range(learning_routine.num_epochs):\n",
    "    for epoch in range(2):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Get data on the devices\n",
    "            graph = batch['graph']\n",
    "            graph = learning_utils.send_graph_to_device(graph, device)\n",
    "\n",
    "            # Do the computations for the forward pass\n",
    "            out = model(graph)\n",
    "            labels = graph.ndata['nt_targets']\n",
    "            loss = torch.nn.MSELoss()(out, labels)\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Metrics\n",
    "            loss = loss.item()\n",
    "            running_loss += loss\n",
    "            losses.append(loss)\n",
    "\n",
    "            if batch_idx % learning_routine.print_each == 0:\n",
    "                time_elapsed = time.time() - start_time\n",
    "                print(\n",
    "                    f'Train Epoch: {epoch + 1} [{(batch_idx + 1)}/{num_batches} '\n",
    "                    f'({100. * (batch_idx + 1) / num_batches:.0f}%)]\\t'\n",
    "                    f'Loss: {loss:.6f}  Time: {time_elapsed:.2f}')\n",
    "\n",
    "                # tensorboard logging\n",
    "                if learning_routine.writer is not None:\n",
    "                    step = epoch * num_batches + batch_idx\n",
    "                    learning_routine.writer.add_scalar(\"Training loss\", loss, step)\n",
    "\n",
    "        train_loss = running_loss / num_batches\n",
    "        if learning_routine.writer is not None:\n",
    "            learning_routine.writer.add_scalar(\"Training epoch loss\", train_loss, epoch)\n",
    "\n",
    "        # Test phase, if we do not have a validation, just iterate.\n",
    "        # Otherwise call the routines.\n",
    "        if learning_routine.validation_loader is None:\n",
    "            early_stop = learning_routine.early_stopping_routine(validation_loss=train_loss, epoch=epoch,\n",
    "                                                                 model=model, optimizer=optimizer)\n",
    "        else:\n",
    "            validation_loss = learning_utils.evaluate_model_supervised(model,\n",
    "                                                                       loader=learning_routine.validation_loader)\n",
    "            if learning_routine.writer is not None:\n",
    "                learning_routine.writer.add_scalar(\"Validation loss during training\", validation_loss, epoch)\n",
    "            early_stop = learning_routine.early_stopping_routine(validation_loss=validation_loss, epoch=epoch,\n",
    "                                                                 model=model, optimizer=optimizer)\n",
    "        if early_stop:\n",
    "            break\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return learning_routine.best_loss\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import torch\n",
    "\n",
    "from rnaglib.learning import models, learn\n",
    "from rnaglib.data_loading import rna_dataset, rna_loader\n",
    "from rnaglib.representations import GraphRepresentation\n",
    "\n",
    "\"\"\"\n",
    "This script just shows a first very basic example : learn binding protein preferences \n",
    "from the nucleotide types and the graph structure\n",
    "\n",
    "To do so, we choose our data, create a data loader around it, build a RGCN model and train it.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose the data, features and targets to use and GET THE DATA GOING\n",
    "    node_features = ['nt_code']\n",
    "    node_target = ['binding_protein']\n",
    "    graph_rep = GraphRepresentation(framework='dgl')\n",
    "    supervised_dataset = rna_dataset.RNADataset(nt_features=node_features, nt_targets=node_target,\n",
    "                                                representations=[graph_rep])\n",
    "    train_loader, validation_loader, test_loader = rna_loader.get_loader(dataset=supervised_dataset)\n",
    "\n",
    "    # Define a model, we first embed our data in 10 dimensions, and then add one classification\n",
    "    input_dim, target_dim = supervised_dataset.input_dim, supervised_dataset.output_dim\n",
    "    embedder_model = models.Embedder(dims=[10, 10], infeatures_dim=input_dim)\n",
    "    classifier_model = models.Classifier(embedder=embedder_model, classif_dims=[target_dim])\n",
    "\n",
    "    # Finally get the training going\n",
    "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.5)\n",
    "    train_supervised(model=classifier_model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to pretrain the network\n",
      "Dataset was found and not overwritten\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py\", line 99, in __getitem__\n    rna_dict[rep.name] = rep(rna_graph, features_dict)\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/representations/rings.py\", line 27, in __call__\n    raise ValueError(\nValueError: To use rings, one needs to use annotated data. The key graphlet_annots is missing from the graph.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m embedder_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mEmbedder(infeatures_dim\u001b[39m=\u001b[39munsupervised_dataset\u001b[39m.\u001b[39minput_dim,\n\u001b[1;32m     32\u001b[0m                                  dims\u001b[39m=\u001b[39m[\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m])\n\u001b[1;32m     33\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(embedder_model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m---> 34\u001b[0m learn\u001b[39m.\u001b[39;49mpretrain_unsupervised(model\u001b[39m=\u001b[39;49membedder_model,\n\u001b[1;32m     35\u001b[0m                             optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     36\u001b[0m                             train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m     37\u001b[0m                             learning_routine\u001b[39m=\u001b[39;49mlearning_utils\u001b[39m.\u001b[39;49mLearningRoutine(num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m),\n\u001b[1;32m     38\u001b[0m                             rec_params\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39msimilarity\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mnormalize\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39muse_graph\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mhops\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m2\u001b[39;49m})\n\u001b[1;32m     39\u001b[0m \u001b[39m# torch.save(embedder_model.state_dict(), 'pretrained_model.pth')\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/learning/learn.py:51\u001b[0m, in \u001b[0;36mpretrain_unsupervised\u001b[0;34m(model, train_loader, optimizer, node_sim, learning_routine, rec_params)\u001b[0m\n\u001b[1;32m     49\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     50\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m---> 51\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     52\u001b[0m     graph, (K, node_ids) \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mgraphs\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mring\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     53\u001b[0m     \u001b[39m# Get data on the devices\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py\", line 99, in __getitem__\n    rna_dict[rep.name] = rep(rna_graph, features_dict)\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/representations/rings.py\", line 27, in __call__\n    raise ValueError(\nValueError: To use rings, one needs to use annotated data. The key graphlet_annots is missing from the graph.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "\n",
    "from rnaglib.kernels import node_sim\n",
    "from rnaglib.data_loading import rna_dataset, rna_loader\n",
    "from rnaglib.representations import GraphRepresentation, RingRepresentation\n",
    "from rnaglib.learning import models, learning_utils, learn\n",
    "\n",
    "\"\"\"\n",
    "This script shows a second more complicated example : learn binding protein preferences as well as\n",
    "small molecules binding from the nucleotide types and the graph structure\n",
    "We also add a pretraining phase based on the R_graphlets kernel\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose the data, features and targets to use\n",
    "    node_features = ['nt_code']\n",
    "    node_target = ['binding_protein']\n",
    "\n",
    "    ###### Unsupervised phase : ######\n",
    "    # Choose the data and kernel to use for pretraining\n",
    "    print('Starting to pretrain the network')\n",
    "    node_simfunc = node_sim.SimFunctionNode(method='R_graphlets', depth=2)\n",
    "    graph_representation = GraphRepresentation(framework='dgl')\n",
    "    ring_representation = RingRepresentation(node_simfunc=node_simfunc, max_size_kernel=50)\n",
    "    unsupervised_dataset = rna_dataset.RNADataset(nt_features=node_features,\n",
    "                                                  representations=[ring_representation, graph_representation])\n",
    "    train_loader = rna_loader.get_loader(dataset=unsupervised_dataset, split=False, num_workers=4)\n",
    "\n",
    "    # Then choose the embedder model and pre_train it, we dump a version of this pretrained model\n",
    "    embedder_model = models.Embedder(infeatures_dim=unsupervised_dataset.input_dim,\n",
    "                                     dims=[64, 64])\n",
    "    optimizer = torch.optim.Adam(embedder_model.parameters())\n",
    "    learn.pretrain_unsupervised(model=embedder_model,\n",
    "                                optimizer=optimizer,\n",
    "                                train_loader=train_loader,\n",
    "                                learning_routine=learning_utils.LearningRoutine(num_epochs=10),\n",
    "                                rec_params={\"similarity\": True, \"normalize\": False, \"use_graph\": True, \"hops\": 2})\n",
    "    # torch.save(embedder_model.state_dict(), 'pretrained_model.pth')\n",
    "    print()\n",
    "\n",
    "    ###### Now the supervised phase : ######\n",
    "    print('We have finished pretraining the network, let us fine tune it')\n",
    "    # GET THE DATA GOING, we want to use precise data splits to be able to use the benchmark.\n",
    "    supervised_train_dataset = rna_dataset.RNADataset(nt_features=node_features,\n",
    "                                                      nt_targets=node_target,\n",
    "                                                      representations=[graph_representation])\n",
    "    train_loader, _, test_loader = rna_loader.get_loader(dataset=supervised_train_dataset,\n",
    "                                                         split_train=0.8, split_valid=0.8,\n",
    "                                                         num_workers=10)\n",
    "\n",
    "    # Define a model and train it :\n",
    "    # We first embed our data in 64 dimensions, using the pretrained embedder and then add one classification\n",
    "    # Then get the training going\n",
    "    classifier_model = models.Classifier(embedder=embedder_model, classif_dims=[supervised_train_dataset.output_dim])\n",
    "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.001)\n",
    "    learn.train_supervised(model=classifier_model,\n",
    "                           optimizer=optimizer,\n",
    "                           train_loader=train_loader,\n",
    "                           learning_routine=learning_utils.LearningRoutine(num_epochs=10))\n",
    "\n",
    "    # Get a benchmark performance on the official uncontaminated test set :\n",
    "    metric = learning_utils.evaluate_model_supervised(model=classifier_model, loader=test_loader)\n",
    "    print('We get a performance of :', metric)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
