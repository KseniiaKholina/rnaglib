{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapted from rnaglib_first\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_supervised function from learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was found and not overwritten\n",
      "Train Epoch: 1 [1/84 (1%)]\tLoss: 3.262544  Time: 0.73\n",
      "Train Epoch: 1 [21/84 (25%)]\tLoss: 1.723011  Time: 4.57\n",
      "Train Epoch: 1 [41/84 (49%)]\tLoss: 0.957033  Time: 9.99\n",
      "Train Epoch: 1 [61/84 (73%)]\tLoss: 0.760158  Time: 12.97\n",
      "Train Epoch: 1 [81/84 (96%)]\tLoss: 0.645697  Time: 18.61\n",
      "Train Epoch: 2 [1/84 (1%)]\tLoss: 0.699450  Time: 19.39\n",
      "Train Epoch: 2 [21/84 (25%)]\tLoss: 0.558471  Time: 24.70\n",
      "Train Epoch: 2 [41/84 (49%)]\tLoss: 0.528238  Time: 29.67\n",
      "Train Epoch: 2 [61/84 (73%)]\tLoss: 0.463922  Time: 34.21\n",
      "Train Epoch: 2 [81/84 (96%)]\tLoss: 0.463149  Time: 37.37\n",
      "Train Epoch: 3 [1/84 (1%)]\tLoss: 0.480446  Time: 37.62\n",
      "Train Epoch: 3 [21/84 (25%)]\tLoss: 0.367840  Time: 43.04\n",
      "Train Epoch: 3 [41/84 (49%)]\tLoss: 0.394295  Time: 46.00\n",
      "Train Epoch: 3 [61/84 (73%)]\tLoss: 0.358360  Time: 51.15\n",
      "Train Epoch: 3 [81/84 (96%)]\tLoss: 0.361634  Time: 55.39\n",
      "Train Epoch: 4 [1/84 (1%)]\tLoss: 0.360547  Time: 56.11\n",
      "Train Epoch: 4 [21/84 (25%)]\tLoss: 0.285045  Time: 62.30\n",
      "Train Epoch: 4 [41/84 (49%)]\tLoss: 0.331315  Time: 65.21\n",
      "Train Epoch: 4 [61/84 (73%)]\tLoss: 0.307029  Time: 70.26\n",
      "Train Epoch: 4 [81/84 (96%)]\tLoss: 0.331870  Time: 74.96\n",
      "Train Epoch: 5 [1/84 (1%)]\tLoss: 0.327473  Time: 75.89\n",
      "Train Epoch: 5 [21/84 (25%)]\tLoss: 0.302848  Time: 79.60\n",
      "Train Epoch: 5 [41/84 (49%)]\tLoss: 0.284440  Time: 84.30\n",
      "Train Epoch: 5 [61/84 (73%)]\tLoss: 0.288893  Time: 91.08\n",
      "Train Epoch: 5 [81/84 (96%)]\tLoss: 0.344019  Time: 94.56\n",
      "Train Epoch: 6 [1/84 (1%)]\tLoss: 0.306441  Time: 95.67\n",
      "Train Epoch: 6 [21/84 (25%)]\tLoss: 0.271887  Time: 100.40\n",
      "Train Epoch: 6 [41/84 (49%)]\tLoss: 0.299849  Time: 104.03\n",
      "Train Epoch: 6 [61/84 (73%)]\tLoss: 0.270416  Time: 109.55\n",
      "Train Epoch: 6 [81/84 (96%)]\tLoss: 0.284325  Time: 113.21\n",
      "Train Epoch: 7 [1/84 (1%)]\tLoss: 0.276494  Time: 113.64\n",
      "Train Epoch: 7 [21/84 (25%)]\tLoss: 0.268171  Time: 117.12\n",
      "Train Epoch: 7 [41/84 (49%)]\tLoss: 0.255221  Time: 121.39\n",
      "Train Epoch: 7 [61/84 (73%)]\tLoss: 0.257375  Time: 126.28\n",
      "Train Epoch: 7 [81/84 (96%)]\tLoss: 0.280025  Time: 131.15\n",
      "Train Epoch: 8 [1/84 (1%)]\tLoss: 0.275283  Time: 132.77\n",
      "Train Epoch: 8 [21/84 (25%)]\tLoss: 0.323432  Time: 138.17\n",
      "Train Epoch: 8 [41/84 (49%)]\tLoss: 0.250309  Time: 141.79\n",
      "Train Epoch: 8 [61/84 (73%)]\tLoss: 0.276569  Time: 146.01\n",
      "Train Epoch: 8 [81/84 (96%)]\tLoss: 0.261836  Time: 149.34\n",
      "Train Epoch: 9 [1/84 (1%)]\tLoss: 0.271841  Time: 150.92\n",
      "Train Epoch: 9 [21/84 (25%)]\tLoss: 0.223068  Time: 156.93\n",
      "Train Epoch: 9 [41/84 (49%)]\tLoss: 0.260160  Time: 161.73\n",
      "Train Epoch: 9 [61/84 (73%)]\tLoss: 0.247900  Time: 165.87\n",
      "Train Epoch: 9 [81/84 (96%)]\tLoss: 0.246266  Time: 169.16\n",
      "Train Epoch: 10 [1/84 (1%)]\tLoss: 0.217010  Time: 169.58\n",
      "Train Epoch: 10 [21/84 (25%)]\tLoss: 0.269694  Time: 174.06\n",
      "Train Epoch: 10 [41/84 (49%)]\tLoss: 0.238972  Time: 179.60\n",
      "Train Epoch: 10 [61/84 (73%)]\tLoss: 0.257833  Time: 183.54\n",
      "Train Epoch: 10 [81/84 (96%)]\tLoss: 0.241360  Time: 187.33\n",
      "Train Epoch: 11 [1/84 (1%)]\tLoss: 0.256916  Time: 188.68\n",
      "Train Epoch: 11 [21/84 (25%)]\tLoss: 0.243930  Time: 193.43\n",
      "Train Epoch: 11 [41/84 (49%)]\tLoss: 0.227433  Time: 197.79\n",
      "Train Epoch: 11 [61/84 (73%)]\tLoss: 0.233123  Time: 203.51\n",
      "Train Epoch: 11 [81/84 (96%)]\tLoss: 0.267710  Time: 206.92\n",
      "Train Epoch: 12 [1/84 (1%)]\tLoss: 0.269421  Time: 208.17\n",
      "Train Epoch: 12 [21/84 (25%)]\tLoss: 0.257013  Time: 211.38\n",
      "Train Epoch: 12 [41/84 (49%)]\tLoss: 0.232684  Time: 216.87\n",
      "Train Epoch: 12 [61/84 (73%)]\tLoss: 0.245535  Time: 221.27\n",
      "Train Epoch: 12 [81/84 (96%)]\tLoss: 0.204360  Time: 224.82\n",
      "Train Epoch: 13 [1/84 (1%)]\tLoss: 0.220160  Time: 225.83\n",
      "Train Epoch: 13 [21/84 (25%)]\tLoss: 0.251767  Time: 229.75\n",
      "Train Epoch: 13 [41/84 (49%)]\tLoss: 0.258738  Time: 235.00\n",
      "Train Epoch: 13 [61/84 (73%)]\tLoss: 0.260180  Time: 239.27\n",
      "Train Epoch: 13 [81/84 (96%)]\tLoss: 0.248169  Time: 243.69\n",
      "Train Epoch: 14 [1/84 (1%)]\tLoss: 0.267642  Time: 244.32\n",
      "Train Epoch: 14 [21/84 (25%)]\tLoss: 0.223777  Time: 251.14\n",
      "Train Epoch: 14 [41/84 (49%)]\tLoss: 0.249364  Time: 256.22\n",
      "Train Epoch: 14 [61/84 (73%)]\tLoss: 0.244116  Time: 259.75\n",
      "Train Epoch: 14 [81/84 (96%)]\tLoss: 0.198428  Time: 262.77\n",
      "Train Epoch: 15 [1/84 (1%)]\tLoss: 0.221320  Time: 263.11\n",
      "Train Epoch: 15 [21/84 (25%)]\tLoss: 0.231482  Time: 267.91\n",
      "Train Epoch: 15 [41/84 (49%)]\tLoss: 0.243091  Time: 270.71\n",
      "Train Epoch: 15 [61/84 (73%)]\tLoss: 0.219994  Time: 276.46\n",
      "Train Epoch: 15 [81/84 (96%)]\tLoss: 0.257151  Time: 281.12\n",
      "Train Epoch: 16 [1/84 (1%)]\tLoss: 0.227676  Time: 281.61\n",
      "Train Epoch: 16 [21/84 (25%)]\tLoss: 0.176546  Time: 285.89\n",
      "Train Epoch: 16 [41/84 (49%)]\tLoss: 0.258402  Time: 291.10\n",
      "Train Epoch: 16 [61/84 (73%)]\tLoss: 0.271479  Time: 294.51\n",
      "Train Epoch: 16 [81/84 (96%)]\tLoss: 0.235317  Time: 299.60\n",
      "Train Epoch: 17 [1/84 (1%)]\tLoss: 0.237026  Time: 300.13\n",
      "Train Epoch: 17 [21/84 (25%)]\tLoss: 0.217676  Time: 304.89\n",
      "Train Epoch: 17 [41/84 (49%)]\tLoss: 0.256344  Time: 309.60\n",
      "Train Epoch: 17 [61/84 (73%)]\tLoss: 0.228700  Time: 312.62\n",
      "Train Epoch: 17 [81/84 (96%)]\tLoss: 0.242061  Time: 317.00\n",
      "Train Epoch: 18 [1/84 (1%)]\tLoss: 0.238496  Time: 318.62\n",
      "Train Epoch: 18 [21/84 (25%)]\tLoss: 0.252040  Time: 323.69\n",
      "Train Epoch: 18 [41/84 (49%)]\tLoss: 0.251883  Time: 328.28\n",
      "Train Epoch: 18 [61/84 (73%)]\tLoss: 0.250165  Time: 333.49\n",
      "Train Epoch: 18 [81/84 (96%)]\tLoss: 0.222991  Time: 336.48\n",
      "Train Epoch: 19 [1/84 (1%)]\tLoss: 0.207765  Time: 337.05\n",
      "Train Epoch: 19 [21/84 (25%)]\tLoss: 0.250435  Time: 342.40\n",
      "Train Epoch: 19 [41/84 (49%)]\tLoss: 0.222524  Time: 347.49\n",
      "Train Epoch: 19 [61/84 (73%)]\tLoss: 0.226418  Time: 351.15\n",
      "Train Epoch: 19 [81/84 (96%)]\tLoss: 0.212203  Time: 355.20\n",
      "Train Epoch: 20 [1/84 (1%)]\tLoss: 0.254145  Time: 355.66\n",
      "Train Epoch: 20 [21/84 (25%)]\tLoss: 0.192451  Time: 359.04\n",
      "Train Epoch: 20 [41/84 (49%)]\tLoss: 0.205631  Time: 363.53\n",
      "Train Epoch: 20 [61/84 (73%)]\tLoss: 0.218868  Time: 367.01\n",
      "Train Epoch: 20 [81/84 (96%)]\tLoss: 0.234060  Time: 372.55\n",
      "Train Epoch: 21 [1/84 (1%)]\tLoss: 0.224509  Time: 373.80\n",
      "Train Epoch: 21 [21/84 (25%)]\tLoss: 0.226369  Time: 377.10\n",
      "Train Epoch: 21 [41/84 (49%)]\tLoss: 0.234973  Time: 381.42\n",
      "Train Epoch: 21 [61/84 (73%)]\tLoss: 0.212816  Time: 386.00\n",
      "Train Epoch: 21 [81/84 (96%)]\tLoss: 0.235557  Time: 391.82\n",
      "Train Epoch: 22 [1/84 (1%)]\tLoss: 0.236486  Time: 392.16\n",
      "Train Epoch: 22 [21/84 (25%)]\tLoss: 0.233830  Time: 395.69\n",
      "Train Epoch: 22 [41/84 (49%)]\tLoss: 0.231469  Time: 401.06\n",
      "Train Epoch: 22 [61/84 (73%)]\tLoss: 0.236709  Time: 404.80\n",
      "Train Epoch: 22 [81/84 (96%)]\tLoss: 0.233050  Time: 409.76\n",
      "Train Epoch: 23 [1/84 (1%)]\tLoss: 0.248601  Time: 411.29\n",
      "Train Epoch: 23 [21/84 (25%)]\tLoss: 0.230360  Time: 415.18\n",
      "Train Epoch: 23 [41/84 (49%)]\tLoss: 0.248914  Time: 420.73\n",
      "Train Epoch: 23 [61/84 (73%)]\tLoss: 0.206108  Time: 425.08\n",
      "Train Epoch: 23 [81/84 (96%)]\tLoss: 0.324184  Time: 429.37\n",
      "Train Epoch: 24 [1/84 (1%)]\tLoss: 0.255266  Time: 430.21\n",
      "Train Epoch: 24 [21/84 (25%)]\tLoss: 0.222023  Time: 434.90\n",
      "Train Epoch: 24 [41/84 (49%)]\tLoss: 0.240102  Time: 440.16\n",
      "Train Epoch: 24 [61/84 (73%)]\tLoss: 0.231092  Time: 443.36\n",
      "Train Epoch: 24 [81/84 (96%)]\tLoss: 0.209837  Time: 448.03\n",
      "Train Epoch: 25 [1/84 (1%)]\tLoss: 0.220687  Time: 448.60\n",
      "Train Epoch: 25 [21/84 (25%)]\tLoss: 0.258035  Time: 453.29\n",
      "Train Epoch: 25 [41/84 (49%)]\tLoss: 0.257048  Time: 457.66\n",
      "Train Epoch: 25 [61/84 (73%)]\tLoss: 0.174323  Time: 462.60\n",
      "Train Epoch: 25 [81/84 (96%)]\tLoss: 0.189117  Time: 477.16\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import torch\n",
    "\n",
    "from rnaglib.learning import models, learn\n",
    "from rnaglib.data_loading import rna_dataset, rna_loader\n",
    "from rnaglib.representations import GraphRepresentation\n",
    "\n",
    "\"\"\"\n",
    "This script just shows a first very basic example : learn binding protein preferences \n",
    "from the nucleotide types and the graph structure\n",
    "\n",
    "To do so, we choose our data, create a data loader around it, build a RGCN model and train it.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose the data, features and targets to use and GET THE DATA GOING\n",
    "    node_features = ['nt_code']\n",
    "    node_target = ['binding_protein']\n",
    "    graph_rep = GraphRepresentation(framework='dgl')\n",
    "    supervised_dataset = rna_dataset.RNADataset(nt_features=node_features, nt_targets=node_target,\n",
    "                                                representations=[graph_rep])\n",
    "    train_loader, validation_loader, test_loader = rna_loader.get_loader(dataset=supervised_dataset)\n",
    "\n",
    "    # Define a model, we first embed our data in 10 dimensions, and then add one classification\n",
    "    input_dim, target_dim = supervised_dataset.input_dim, supervised_dataset.output_dim\n",
    "    embedder_model = models.Embedder(dims=[10, 10], infeatures_dim=input_dim)\n",
    "    classifier_model = models.Classifier(embedder=embedder_model, classif_dims=[target_dim])\n",
    "\n",
    "    # Finally get the training going\n",
    "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.001)\n",
    "    learn.train_supervised(model=classifier_model,\n",
    "                           optimizer=optimizer,\n",
    "                           train_loader=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to pretrain the network\n",
      "Dataset was found and not overwritten\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py\", line 99, in __getitem__\n    rna_dict[rep.name] = rep(rna_graph, features_dict)\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/representations/rings.py\", line 27, in __call__\n    raise ValueError(\nValueError: To use rings, one needs to use annotated data. The key graphlet_annots is missing from the graph.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m embedder_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mEmbedder(infeatures_dim\u001b[39m=\u001b[39munsupervised_dataset\u001b[39m.\u001b[39minput_dim,\n\u001b[1;32m     32\u001b[0m                                  dims\u001b[39m=\u001b[39m[\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m])\n\u001b[1;32m     33\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(embedder_model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m---> 34\u001b[0m learn\u001b[39m.\u001b[39;49mpretrain_unsupervised(model\u001b[39m=\u001b[39;49membedder_model,\n\u001b[1;32m     35\u001b[0m                             optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     36\u001b[0m                             train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m     37\u001b[0m                             learning_routine\u001b[39m=\u001b[39;49mlearning_utils\u001b[39m.\u001b[39;49mLearningRoutine(num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m),\n\u001b[1;32m     38\u001b[0m                             rec_params\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39msimilarity\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mnormalize\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39muse_graph\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mhops\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m2\u001b[39;49m})\n\u001b[1;32m     39\u001b[0m \u001b[39m# torch.save(embedder_model.state_dict(), 'pretrained_model.pth')\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/learning/learn.py:51\u001b[0m, in \u001b[0;36mpretrain_unsupervised\u001b[0;34m(model, train_loader, optimizer, node_sim, learning_routine, rec_params)\u001b[0m\n\u001b[1;32m     49\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     50\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m---> 51\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     52\u001b[0m     graph, (K, node_ids) \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mgraphs\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mring\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     53\u001b[0m     \u001b[39m# Get data on the devices\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/data_loading/rna_dataset.py\", line 99, in __getitem__\n    rna_dict[rep.name] = rep(rna_graph, features_dict)\n  File \"/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/rnaglib/representations/rings.py\", line 27, in __call__\n    raise ValueError(\nValueError: To use rings, one needs to use annotated data. The key graphlet_annots is missing from the graph.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "\n",
    "from rnaglib.kernels import node_sim\n",
    "from rnaglib.data_loading import rna_dataset, rna_loader\n",
    "from rnaglib.representations import GraphRepresentation, RingRepresentation\n",
    "from rnaglib.learning import models, learning_utils, learn\n",
    "\n",
    "\"\"\"\n",
    "This script shows a second more complicated example : learn binding protein preferences as well as\n",
    "small molecules binding from the nucleotide types and the graph structure\n",
    "We also add a pretraining phase based on the R_graphlets kernel\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose the data, features and targets to use\n",
    "    node_features = ['nt_code']\n",
    "    node_target = ['binding_protein']\n",
    "\n",
    "    ###### Unsupervised phase : ######\n",
    "    # Choose the data and kernel to use for pretraining\n",
    "    print('Starting to pretrain the network')\n",
    "    node_simfunc = node_sim.SimFunctionNode(method='R_graphlets', depth=2)\n",
    "    graph_representation = GraphRepresentation(framework='dgl')\n",
    "    ring_representation = RingRepresentation(node_simfunc=node_simfunc, max_size_kernel=50)\n",
    "    unsupervised_dataset = rna_dataset.RNADataset(nt_features=node_features,\n",
    "                                                  representations=[ring_representation, graph_representation])\n",
    "    train_loader = rna_loader.get_loader(dataset=unsupervised_dataset, split=False, num_workers=4)\n",
    "\n",
    "    # Then choose the embedder model and pre_train it, we dump a version of this pretrained model\n",
    "    embedder_model = models.Embedder(infeatures_dim=unsupervised_dataset.input_dim,\n",
    "                                     dims=[64, 64])\n",
    "    optimizer = torch.optim.Adam(embedder_model.parameters())\n",
    "    learn.pretrain_unsupervised(model=embedder_model,\n",
    "                                optimizer=optimizer,\n",
    "                                train_loader=train_loader,\n",
    "                                learning_routine=learning_utils.LearningRoutine(num_epochs=10),\n",
    "                                rec_params={\"similarity\": True, \"normalize\": False, \"use_graph\": True, \"hops\": 2})\n",
    "    # torch.save(embedder_model.state_dict(), 'pretrained_model.pth')\n",
    "    print()\n",
    "\n",
    "    ###### Now the supervised phase : ######\n",
    "    print('We have finished pretraining the network, let us fine tune it')\n",
    "    # GET THE DATA GOING, we want to use precise data splits to be able to use the benchmark.\n",
    "    supervised_train_dataset = rna_dataset.RNADataset(nt_features=node_features,\n",
    "                                                      nt_targets=node_target,\n",
    "                                                      representations=[graph_representation])\n",
    "    train_loader, _, test_loader = rna_loader.get_loader(dataset=supervised_train_dataset,\n",
    "                                                         split_train=0.8, split_valid=0.8,\n",
    "                                                         num_workers=10)\n",
    "\n",
    "    # Define a model and train it :\n",
    "    # We first embed our data in 64 dimensions, using the pretrained embedder and then add one classification\n",
    "    # Then get the training going\n",
    "    classifier_model = models.Classifier(embedder=embedder_model, classif_dims=[supervised_train_dataset.output_dim])\n",
    "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.001)\n",
    "    learn.train_supervised(model=classifier_model,\n",
    "                           optimizer=optimizer,\n",
    "                           train_loader=train_loader,\n",
    "                           learning_routine=learning_utils.LearningRoutine(num_epochs=10))\n",
    "\n",
    "    # Get a benchmark performance on the official uncontaminated test set :\n",
    "    metric = learning_utils.evaluate_model_supervised(model=classifier_model, loader=test_loader)\n",
    "    print('We get a performance of :', metric)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
